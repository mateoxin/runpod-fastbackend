#!/usr/bin/env python3
"""
Tworzenie DZIAÅAJÄ„CEGO RunPod Endpoint
Z najlepszym dostÄ™pnym GPU i gwarantowanym active worker
"""

import runpod
import os
import json
import time
from dotenv import load_dotenv
from datetime import datetime

def setup_runpod():
    """Konfiguracja RunPod"""
    load_dotenv('config.env')
    runpod.api_key = os.getenv('RUNPOD_API_KEY')
    
    if not runpod.api_key:
        raise ValueError("RUNPOD_API_KEY nie jest ustawiony!")
    
    print(f"âœ… RunPod API skonfigurowany: {runpod.api_key[:10]}...")
    return True

def find_best_available_gpu():
    """Znajduje najlepszÄ… dostÄ™pnÄ… kartÄ™ GPU"""
    
    print("\nğŸ” Szukanie najlepszej dostÄ™pnej karty GPU...")
    
    # Lista preferencji GPU (od najlepszych)
    gpu_preferences = [
        'NVIDIA GeForce RTX 5090',  # 32GB VRAM - najnowsza
        'NVIDIA GeForce RTX 4090',  # 24GB VRAM - sprawdzona
        'NVIDIA GeForce RTX 3090',  # 24GB VRAM - nasza pierwotna opcja
        'NVIDIA GeForce RTX 4080 SUPER',  # 16GB VRAM
        'NVIDIA GeForce RTX 4080',  # 16GB VRAM  
        'NVIDIA RTX A5000',        # 24GB VRAM - workstation
        'NVIDIA RTX A4500'         # 20GB VRAM - backup
    ]
    
    try:
        gpus = runpod.get_gpus()
        available_gpus = {gpu['id']: gpu for gpu in gpus}
        
        print("ğŸ“Š Sprawdzanie preferencji GPU:")
        for gpu_id in gpu_preferences:
            if gpu_id in available_gpus:
                gpu_info = available_gpus[gpu_id]
                print(f"  âœ… {gpu_info['displayName']} ({gpu_info['memoryInGb']}GB) - DOSTÄ˜PNE")
                return gpu_id, gpu_info
            else:
                print(f"  âŒ {gpu_id} - niedostÄ™pne")
        
        print("âš ï¸  Å»adne preferowane GPU nie jest dostÄ™pne, uÅ¼ywam pierwszego dostÄ™pnego...")
        if gpus:
            gpu_info = gpus[0]
            return gpu_info['id'], gpu_info
        else:
            print("âŒ Brak dostÄ™pnych GPU!")
            return None, None
            
    except Exception as e:
        print(f"âŒ BÅ‚Ä…d sprawdzania GPU: {e}")
        return None, None

def create_working_template(gpu_info):
    """Tworzy template z optymalizacjami dla wybranego GPU"""
    
    gpu_name = gpu_info['displayName'] if gpu_info else 'Unknown'
    gpu_memory = gpu_info['memoryInGb'] if gpu_info else 'Unknown'
    
    # Zmienne Å›rodowiskowe
    env_vars = {
        'RUNPOD_API_KEY': os.getenv('RUNPOD_API_KEY'),
        'HF_TOKEN': os.getenv('HF_TOKEN'), 
        'GITHUB_TOKEN': os.getenv('GITHUB_TOKEN'),
        'PYTHONUNBUFFERED': '1',
        'HANDLER_FILE': 'handler_fast.py',
        'CUDA_VISIBLE_DEVICES': '0',
        'NVIDIA_VISIBLE_DEVICES': 'all',
        'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility',
        'RUNPOD_INIT_TIMEOUT': '900',   # 15 min timeout
        'MAX_WORKERS': '1',
        'TIMEOUT': '1200',  # 20 min timeout
        'MEMORY_LIMIT': '100G'
    }
    
    print(f"\nğŸ“‹ Tworzenie template dla {gpu_name}...")
    print("ğŸ”§ Konfiguracja:")
    print("   - Image: PyTorch 2.1.0 + CUDA 11.8")
    print("   - Container Disk: 100GB")
    print("   - Volume: 100GB") 
    print(f"   - Target GPU: {gpu_name} ({gpu_memory}GB)")
    print("   - Init timeout: 15 min")
    print(f"   - Env vars: {len(env_vars)}")
    
    try:
        template_response = runpod.create_template(
            name=f'fastbackend-{gpu_name.lower().replace(" ", "-")}-template',
            image_name='runpod/pytorch:2.1.0-py3.10-cuda11.8.0-devel-ubuntu22.04',
            docker_start_cmd='python3 handler_fast.py',
            container_disk_in_gb=100,
            volume_in_gb=100,
            volume_mount_path='/workspace',
            ports='8000/http,22/tcp',
            env=env_vars,
            is_serverless=True
        )
        
        if template_response:
            template_id = template_response.get('id')
            print(f"âœ… Template utworzony: {template_id}")
            return template_id
        else:
            print("âŒ BÅ‚Ä…d tworzenia template")
            return None
            
    except Exception as e:
        print(f"âŒ BÅ‚Ä…d: {e}")
        return None

def create_guaranteed_endpoint(template_id, gpu_id, gpu_info):
    """Tworzy endpoint z gwarancjÄ… dziaÅ‚ania"""
    
    gpu_name = gpu_info['displayName']
    gpu_memory = gpu_info['memoryInGb']
    
    print(f"\nğŸš€ Tworzenie GWARANTOWANEGO endpoint...")
    print("ğŸ“Š Optymalne parametry:")
    print(f"   - GPU: {gpu_name} ({gpu_memory}GB VRAM)")
    print("   - Template: Zoptymalizowany")
    print("   - â­ WORKERS_MIN: 1 (ACTIVE WORKER)")
    print("   - Workers max: 1")
    print("   - Storage: 100GB")
    print("   - Idle timeout: 60s (dÅ‚uÅ¼szy)")
    print("   - FlashBoot: enabled")
    print("   - Locations: US,EU (multi-region)")
    
    try:
        response = runpod.create_endpoint(
            name=f'fastbackend-{gpu_name.lower().replace(" ", "-")}-working',
            template_id=template_id,
            gpu_ids=gpu_id,
            locations='US,EU',        # Multi-region dla dostÄ™pnoÅ›ci
            idle_timeout=60,          # DÅ‚uÅ¼szy timeout
            scaler_type='QUEUE_DELAY',
            scaler_value=3,           # Szybka reakcja
            workers_min=1,            # ğŸ¯ ACTIVE WORKER!
            workers_max=1,            # Pojedynczy worker
            gpu_count=1,              # 1 GPU per worker
            flashboot=True            # FlashBoot enabled
        )
        
        if response:
            endpoint_id = response.get('id') if isinstance(response, dict) else response
            
            print(f"\nâœ… WORKING endpoint utworzony!")
            print(f"ğŸ†” ID: {endpoint_id}")
            print(f"ğŸ“› Name: fastbackend-{gpu_name.lower().replace(' ', '-')}-working")
            print(f"ğŸ”— URL: https://api.runpod.ai/v2/{endpoint_id}")
            print(f"â­ ACTIVE WORKER z {gpu_name}!")
            
            # Zapisz informacje
            endpoint_info = {
                'id': endpoint_id,
                'name': f'fastbackend-{gpu_name.lower().replace(" ", "-")}-working',
                'url': f'https://api.runpod.ai/v2/{endpoint_id}',
                'template_id': template_id,
                'gpu_type': gpu_id,
                'gpu_name': gpu_name,
                'gpu_memory': f'{gpu_memory}GB',
                'container_disk_gb': 100,
                'volume_gb': 100,
                'workers_min': 1,
                'workers_max': 1,
                'idle_timeout': 60,
                'flashboot': True,
                'version': 'working',
                'has_active_worker': True,
                'created_at': datetime.now().isoformat(),
                'status': 'creating'
            }
            
            # Zapisz do pliku
            with open('endpoint_working_info.json', 'w') as f:
                json.dump(endpoint_info, f, indent=2)
            
            print(f"ğŸ’¾ Informacje zapisane w endpoint_working_info.json")
            return endpoint_id
            
        else:
            print("âŒ BÅ‚Ä…d tworzenia endpoint")
            return None
            
    except Exception as e:
        print(f"âŒ BÅ‚Ä…d endpoint: {e}")
        return None

def wait_and_verify_worker(endpoint_id, timeout=900):
    """Czeka i weryfikuje uruchomienie active worker"""
    
    print(f"\nâ³ Monitorowanie active worker w endpoint {endpoint_id}...")
    print("ğŸ’¡ Maksymalny czas oczekiwania: 15 minut")
    
    start_time = time.time()
    last_status = None
    
    while time.time() - start_time < timeout:
        try:
            endpoints = runpod.get_endpoints()
            for endpoint in endpoints:
                if endpoint.get('id') == endpoint_id:
                    status = endpoint.get('status', 'Unknown')
                    workers_ready = endpoint.get('workersReady', 0)
                    workers_running = endpoint.get('workersRunning', 0)
                    workers_init = endpoint.get('workersInitializing', 0)
                    
                    # Pokazuj zmiany statusu
                    if status != last_status:
                        print(f"ğŸ“Š Status change: {last_status} â†’ {status}")
                        last_status = status
                    
                    print(f"ğŸ‘¥ Workers - Ready: {workers_ready}, Running: {workers_running}, Init: {workers_init}")
                    
                    if status in ['READY', 'ACTIVE'] and workers_ready > 0:
                        print(f"ğŸ‰ ACTIVE WORKER gotowy! Status: {status}")
                        return True
                    elif status in ['FAILED', 'ERROR']:
                        print(f"âŒ Endpoint failed! Status: {status}")
                        return False
                    elif workers_running > 0 or workers_init > 0:
                        print(f"â³ Worker siÄ™ uruchamia...")
                    
                    break
            
            print("â³ Sprawdzanie za 45s...")
            time.sleep(45)
            
        except Exception as e:
            print(f"âš ï¸  BÅ‚Ä…d monitorowania: {e}")
            time.sleep(45)
    
    print(f"âš ï¸  Timeout po {timeout//60} minutach")
    return False

def main():
    """GÅ‚Ã³wna funkcja"""
    print("ğŸš€ RunPod WORKING Endpoint Creator")
    print("===================================")
    print("ğŸ¯ Cel: Najlepsze GPU + 100GB + GWARANTOWANY ACTIVE WORKER")
    print("ğŸ’¡ Strategia: ZnajdÅº najlepszÄ… dostÄ™pnÄ… kartÄ™ i utwÃ³rz stabilny endpoint")
    print("")
    
    # Setup RunPod
    if not setup_runpod():
        return
    
    # ZnajdÅº najlepsze dostÄ™pne GPU
    gpu_id, gpu_info = find_best_available_gpu()
    if not gpu_id:
        print("âŒ Nie moÅ¼na znaleÅºÄ‡ dostÄ™pnego GPU")
        return
    
    print(f"\nğŸ¯ Wybrane GPU: {gpu_info['displayName']} ({gpu_info['memoryInGb']}GB)")
    
    # UtwÃ³rz template
    template_id = create_working_template(gpu_info)
    if not template_id:
        print("âŒ Nie udaÅ‚o siÄ™ utworzyÄ‡ template")
        return
    
    # UtwÃ³rz working endpoint
    endpoint_id = create_guaranteed_endpoint(template_id, gpu_id, gpu_info)
    if not endpoint_id:
        print("âŒ Nie udaÅ‚o siÄ™ utworzyÄ‡ endpoint")
        return
    
    print(f"\nğŸ¯ WORKING endpoint utworzony!")
    print(f"ğŸ†” ID: {endpoint_id}")
    print(f"ğŸ–¥ï¸  GPU: {gpu_info['displayName']} ({gpu_info['memoryInGb']}GB)")
    print(f"ğŸ“ SzczegÃ³Å‚y w: endpoint_working_info.json")
    print("")
    print("ğŸ“‹ NastÄ™pne kroki:")
    print("1. Active worker uruchomi siÄ™ automatycznie")
    print("2. Poczekaj na gotowoÅ›Ä‡ (maks 15 min)")
    print("3. Uruchom: python test_working_endpoint.py")
    
    # Opcjonalnie czekaj na uruchomienie
    choice = input("\nCzy czekaÄ‡ na uruchomienie active worker? (y/n): ").lower()
    if choice == 'y':
        if wait_and_verify_worker(endpoint_id):
            print("\nğŸ‰ ENDPOINT GOTOWY DO TESTOWANIA!")
        else:
            print("\nâš ï¸  Endpoint moÅ¼e potrzebowaÄ‡ wiÄ™cej czasu")

if __name__ == "__main__":
    main()